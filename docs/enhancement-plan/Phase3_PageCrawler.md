# Phase 3: í˜ì´ì§€ í¬ë¡¤ëŸ¬

**ê¸°ê°„**: 2ì£¼ì°¨
**ìƒíƒœ**: ğŸ“‹ ê³„íš ì™„ë£Œ
**ì˜ì¡´ì„±**: Phase 1 ì™„ë£Œ í•„ìš”

## ëª©í‘œ

ì¸ìš©ëœ URLì˜ ì‹¤ì œ HTML ì½˜í…ì¸ ë¥¼ í˜ì¹­í•˜ê³ , ë©”íƒ€íƒœê·¸, Schema.org ë§ˆí¬ì—…, ì½˜í…ì¸  êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì—¬ ê²½ìŸì‚¬ì™€ì˜ í˜ì´ì§€ êµ¬ì¡° ë¹„êµ ê¸°ë°˜ì„ ë§ˆë ¨í•©ë‹ˆë‹¤.

## í•µì‹¬ ì œì•½ì‚¬í•­

- âœ… **robots.txt ì¡´ì¤‘** (ìœ¤ë¦¬ì  í¬ë¡¤ë§)
- âœ… íƒ€ì„ì•„ì›ƒ: 30ì´ˆ/URL
- âœ… ì†ë„ ì œí•œ: ìµœëŒ€ 10 URLs/ìš”ì²­
- âœ… Content-Length: 5MB ì œí•œ
- âœ… User-Agent: "GEOAnalyzer/1.0 (Educational Research Tool)"

## ì‘ì—… í•­ëª©

### 1. robots.txt ì²´ì»¤

#### íŒŒì¼: `lib/crawler/robots-checker.ts`

```typescript
export interface RobotsCheckResult {
  allowed: boolean
  reason?: string
  robotsTxt?: string
}

/**
 * robots.txtë¥¼ í™•ì¸í•˜ì—¬ í¬ë¡¤ë§ í—ˆìš© ì—¬ë¶€ íŒë‹¨
 */
export async function isAllowedByRobots(url: string): Promise<RobotsCheckResult> {
  try {
    const { origin, pathname } = new URL(url)
    const robotsUrl = `${origin}/robots.txt`

    const controller = new AbortController()
    const timeout = setTimeout(() => controller.abort(), 5000)

    const response = await fetch(robotsUrl, {
      signal: controller.signal,
      headers: {
        'User-Agent': 'GEOAnalyzer/1.0 (Educational Research Tool)'
      }
    })

    clearTimeout(timeout)

    // robots.txtê°€ ì—†ìœ¼ë©´ í—ˆìš©
    if (!response.ok) {
      return { allowed: true, reason: 'No robots.txt found' }
    }

    const robotsTxt = await response.text()

    // ê°„ë‹¨í•œ robots.txt íŒŒì‹±
    const disallowedPaths = parseRobotsTxt(robotsTxt)

    // í˜„ì¬ URLì˜ ê²½ë¡œê°€ disallow ëª©ë¡ì— ìˆëŠ”ì§€ í™•ì¸
    const isDisallowed = disallowedPaths.some(path =>
      pathname.startsWith(path)
    )

    if (isDisallowed) {
      return {
        allowed: false,
        reason: `Disallowed by robots.txt: ${pathname}`,
        robotsTxt
      }
    }

    return { allowed: true, robotsTxt }

  } catch (error) {
    // ì—ëŸ¬ ë°œìƒ ì‹œ ê´€ëŒ€í•˜ê²Œ í—ˆìš©
    return { allowed: true, reason: `robots.txt check failed: ${error.message}` }
  }
}

function parseRobotsTxt(content: string): string[] {
  const lines = content.split('\n')
  const disallowedPaths: string[] = []
  let isWildcardSection = false

  for (const line of lines) {
    const trimmed = line.trim()

    if (trimmed.startsWith('User-agent: *')) {
      isWildcardSection = true
    } else if (trimmed.startsWith('User-agent:')) {
      isWildcardSection = false
    } else if (isWildcardSection && trimmed.startsWith('Disallow:')) {
      const path = trimmed.split('Disallow:')[1].trim()
      if (path && path !== '/') {
        disallowedPaths.push(path)
      }
    }
  }

  return disallowedPaths
}
```

### 2. í˜ì´ì§€ Fetcher

#### íŒŒì¼: `lib/crawler/page-fetcher.ts`

```typescript
import type { MetaTags, ContentStructure } from '@/types/pageCrawl'

export interface PageFetchResult {
  success: boolean
  url: string
  domain: string
  html?: string
  metaTags?: MetaTags
  schemaMarkup?: any[]
  contentStructure?: ContentStructure
  error?: string
  crawlStatus: 'success' | 'failed' | 'blocked_robots'
  robotsAllowed: boolean
}

export async function fetchPage(url: string): Promise<PageFetchResult> {
  const domain = new URL(url).hostname

  try {
    // 1. robots.txt ì²´í¬
    const robotsCheck = await isAllowedByRobots(url)

    if (!robotsCheck.allowed) {
      return {
        success: false,
        url,
        domain,
        error: robotsCheck.reason,
        crawlStatus: 'blocked_robots',
        robotsAllowed: false
      }
    }

    // 2. HTML í˜ì¹­ (30ì´ˆ íƒ€ì„ì•„ì›ƒ)
    const controller = new AbortController()
    const timeout = setTimeout(() => controller.abort(), 30000)

    const response = await fetch(url, {
      signal: controller.signal,
      headers: {
        'User-Agent': 'GEOAnalyzer/1.0 (Educational Research Tool)',
        'Accept': 'text/html',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7'
      }
    })

    clearTimeout(timeout)

    if (!response.ok) {
      return {
        success: false,
        url,
        domain,
        error: `HTTP ${response.status}: ${response.statusText}`,
        crawlStatus: 'failed',
        robotsAllowed: true
      }
    }

    // 3. Content-Length ì²´í¬
    const contentLength = response.headers.get('content-length')
    if (contentLength && parseInt(contentLength) > 5 * 1024 * 1024) {
      return {
        success: false,
        url,
        domain,
        error: 'Content too large (>5MB)',
        crawlStatus: 'failed',
        robotsAllowed: true
      }
    }

    const html = await response.text()

    // 4. HTML íŒŒì‹±ì€ Edge Functionì—ì„œ ìˆ˜í–‰ (Deno DOM)
    return {
      success: true,
      url,
      domain,
      html: html.slice(0, 50000), // ì²˜ìŒ 50KBë§Œ ì €ì¥
      crawlStatus: 'success',
      robotsAllowed: true
    }

  } catch (error) {
    return {
      success: false,
      url,
      domain,
      error: error.message,
      crawlStatus: 'failed',
      robotsAllowed: true
    }
  }
}
```

### 3. Edge Function: crawl-pages

#### íŒŒì¼: `supabase/functions/crawl-pages/index.ts`

```typescript
import { serve } from 'https://deno.land/std@0.168.0/http/server.ts'
import { createClient } from 'https://esm.sh/@supabase/supabase-js@2'
import { DOMParser } from 'https://deno.land/x/deno_dom/deno-dom-wasm.ts'

interface CrawlRequest {
  urls: string[]
  analysisId: string
}

serve(async (req) => {
  try {
    const { urls, analysisId } = await req.json() as CrawlRequest

    if (!urls || urls.length === 0 || urls.length > 10) {
      return new Response(
        JSON.stringify({ error: 'Invalid URLs. Must provide 1-10 URLs' }),
        { status: 400 }
      )
    }

    const supabase = createClient(
      Deno.env.get('SUPABASE_URL')!,
      Deno.env.get('SUPABASE_ANON_KEY')!
    )

    // ê° URL í¬ë¡¤ë§
    const results = await Promise.allSettled(
      urls.map(url => crawlSinglePage(url, analysisId, supabase))
    )

    const crawlResults = results.map((result, index) => ({
      url: urls[index],
      status: result.status,
      data: result.status === 'fulfilled' ? result.value : null,
      error: result.status === 'rejected' ? result.reason : null
    }))

    return new Response(
      JSON.stringify({ results: crawlResults }),
      { headers: { 'Content-Type': 'application/json' } }
    )

  } catch (error) {
    return new Response(
      JSON.stringify({ error: error.message }),
      { status: 500 }
    )
  }
})

async function crawlSinglePage(url: string, analysisId: string, supabase: any) {
  const domain = new URL(url).hostname

  // 1. robots.txt ì²´í¬
  const robotsAllowed = await checkRobotsTxt(url)

  if (!robotsAllowed) {
    await supabase.from('page_crawls').insert({
      analysis_id: analysisId,
      url,
      domain,
      crawl_status: 'blocked_robots',
      robots_txt_allowed: false,
      error_message: 'Blocked by robots.txt'
    })

    return { url, status: 'blocked_robots' }
  }

  // 2. HTML í˜ì¹­
  const controller = new AbortController()
  const timeout = setTimeout(() => controller.abort(), 30000)

  try {
    const response = await fetch(url, {
      signal: controller.signal,
      headers: {
        'User-Agent': 'GEOAnalyzer/1.0 (Educational Research Tool)',
        'Accept': 'text/html'
      }
    })

    clearTimeout(timeout)

    if (!response.ok) {
      throw new Error(`HTTP ${response.status}`)
    }

    const html = await response.text()

    // 3. HTML íŒŒì‹±
    const doc = new DOMParser().parseFromString(html, 'text/html')

    if (!doc) {
      throw new Error('HTML parsing failed')
    }

    // 4. ë©”íƒ€íƒœê·¸ ì¶”ì¶œ
    const metaTags = extractMetaTags(doc)

    // 5. Schema ë§ˆí¬ì—… ì¶”ì¶œ
    const schemaMarkup = extractSchemaMarkup(doc)

    // 6. ì½˜í…ì¸  êµ¬ì¡° ë¶„ì„
    const contentStructure = analyzeContentStructure(doc)

    // 7. DB ì €ì¥
    await supabase.from('page_crawls').insert({
      analysis_id: analysisId,
      url,
      domain,
      crawl_status: 'success',
      html_content: html.slice(0, 50000),
      meta_tags: metaTags,
      schema_markup: schemaMarkup,
      content_structure: contentStructure,
      robots_txt_allowed: true,
      crawled_at: new Date().toISOString()
    })

    return { url, status: 'success' }

  } catch (error) {
    await supabase.from('page_crawls').insert({
      analysis_id: analysisId,
      url,
      domain,
      crawl_status: 'failed',
      error_message: error.message,
      robots_txt_allowed: true
    })

    throw error
  } finally {
    clearTimeout(timeout)
  }
}

async function checkRobotsTxt(url: string): Promise<boolean> {
  try {
    const { origin, pathname } = new URL(url)
    const robotsUrl = `${origin}/robots.txt`

    const response = await fetch(robotsUrl, {
      signal: AbortSignal.timeout(5000)
    })

    if (!response.ok) return true

    const robotsTxt = await response.text()
    const lines = robotsTxt.split('\n')
    let isWildcard = false
    const disallowed: string[] = []

    for (const line of lines) {
      if (line.trim().startsWith('User-agent: *')) {
        isWildcard = true
      } else if (line.trim().startsWith('User-agent:')) {
        isWildcard = false
      } else if (isWildcard && line.trim().startsWith('Disallow:')) {
        const path = line.split('Disallow:')[1].trim()
        if (path) disallowed.push(path)
      }
    }

    return !disallowed.some(path => pathname.startsWith(path))

  } catch {
    return true
  }
}

function extractMetaTags(doc: any) {
  const meta: any = {}

  meta.title = doc.querySelector('title')?.textContent || null

  const metaElements = doc.querySelectorAll('meta')
  for (const el of metaElements) {
    const name = el.getAttribute('name') || el.getAttribute('property')
    const content = el.getAttribute('content')

    if (name && content) {
      if (name === 'description') meta.description = content
      if (name === 'keywords') meta.keywords = content
      if (name === 'author') meta.author = content
      if (name === 'og:title') meta.ogTitle = content
      if (name === 'og:description') meta.ogDescription = content
      if (name === 'og:image') meta.ogImage = content
    }
  }

  const canonical = doc.querySelector('link[rel="canonical"]')
  if (canonical) {
    meta.canonical = canonical.getAttribute('href')
  }

  return meta
}

function extractSchemaMarkup(doc: any) {
  const schemas: any[] = []

  const scripts = doc.querySelectorAll('script[type="application/ld+json"]')
  for (const script of scripts) {
    try {
      const schema = JSON.parse(script.textContent)
      schemas.push(schema)
    } catch {
      // Invalid JSON, skip
    }
  }

  return schemas
}

function analyzeContentStructure(doc: any) {
  const structure: any = {
    headings: { h1: [], h2: [], h3: [] }
  }

  // Headings
  doc.querySelectorAll('h1').forEach((h1: any) => {
    structure.headings.h1.push(h1.textContent?.trim())
  })
  doc.querySelectorAll('h2').forEach((h2: any) => {
    structure.headings.h2.push(h2.textContent?.trim())
  })
  doc.querySelectorAll('h3').forEach((h3: any) => {
    structure.headings.h3.push(h3.textContent?.trim())
  })

  // Counts
  const bodyText = doc.querySelector('body')?.textContent || ''
  structure.wordCount = bodyText.split(/\s+/).length
  structure.paragraphCount = doc.querySelectorAll('p').length
  structure.imageCount = doc.querySelectorAll('img').length
  structure.linkCount = doc.querySelectorAll('a').length

  // Special elements
  structure.hasTableOfContents = !!doc.querySelector('[class*="toc"], [id*="toc"]')
  structure.hasFAQ = !!doc.querySelector('[itemtype*="FAQPage"], [class*="faq"]')

  return structure
}
```

### 4. UI ì»´í¬ë„ŒíŠ¸

#### íŒŒì¼: `components/page-analysis/CrawlResults.tsx`

```typescript
'use client'

import { Badge } from '@/components/ui/badge'
import { Card } from '@/components/ui/card'
import { CheckCircle, XCircle, Ban } from 'lucide-react'
import type { PageCrawl } from '@/types/pageCrawl'

interface CrawlResultsProps {
  crawls: PageCrawl[]
}

export function CrawlResults({ crawls }: CrawlResultsProps) {
  const successCount = crawls.filter(c => c.crawl_status === 'success').length
  const blockedCount = crawls.filter(c => c.crawl_status === 'blocked_robots').length
  const failedCount = crawls.filter(c => c.crawl_status === 'failed').length

  return (
    <div className="space-y-4">
      {/* ìš”ì•½ */}
      <div className="flex gap-4">
        <Badge variant="success">ì„±ê³µ: {successCount}</Badge>
        <Badge variant="warning">ì°¨ë‹¨: {blockedCount}</Badge>
        <Badge variant="destructive">ì‹¤íŒ¨: {failedCount}</Badge>
      </div>

      {/* ê°œë³„ ê²°ê³¼ */}
      <div className="space-y-2">
        {crawls.map((crawl) => (
          <Card key={crawl.id} className="p-4">
            <div className="flex items-start gap-3">
              {/* ìƒíƒœ ì•„ì´ì½˜ */}
              {crawl.crawl_status === 'success' && (
                <CheckCircle className="text-green-500 mt-1" />
              )}
              {crawl.crawl_status === 'blocked_robots' && (
                <Ban className="text-yellow-500 mt-1" />
              )}
              {crawl.crawl_status === 'failed' && (
                <XCircle className="text-red-500 mt-1" />
              )}

              <div className="flex-1">
                <div className="font-medium">{crawl.domain}</div>
                <div className="text-sm text-gray-500 truncate">{crawl.url}</div>

                {/* ì„±ê³µ ì‹œ ë©”íƒ€íƒœê·¸ í‘œì‹œ */}
                {crawl.meta_tags && (
                  <div className="mt-2 text-sm">
                    <div><strong>ì œëª©:</strong> {crawl.meta_tags.title}</div>
                    <div><strong>ì„¤ëª…:</strong> {crawl.meta_tags.description}</div>
                  </div>
                )}

                {/* ì—ëŸ¬ ë©”ì‹œì§€ */}
                {crawl.error_message && (
                  <div className="mt-2 text-sm text-red-600">
                    {crawl.error_message}
                  </div>
                )}
              </div>
            </div>
          </Card>
        ))}
      </div>
    </div>
  )
}
```

## ê²€ì¦ ë°©ë²•

### 1. robots.txt ì²´ì»¤ í…ŒìŠ¤íŠ¸

```typescript
// test-robots.ts
import { isAllowedByRobots } from './lib/crawler/robots-checker'

const testUrls = [
  'https://www.google.com/',
  'https://www.google.com/search', // Disallowed
  'https://example.com/page'
]

for (const url of testUrls) {
  const result = await isAllowedByRobots(url)
  console.log(url, result.allowed ? 'âœ…' : 'âŒ', result.reason)
}
```

### 2. Edge Function í…ŒìŠ¤íŠ¸

```bash
supabase functions serve crawl-pages

curl -X POST 'http://localhost:54321/functions/v1/crawl-pages' \
  -H 'Content-Type: application/json' \
  -d '{"urls":["https://example.com"],"analysisId":"test-id"}'
```

## ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] `lib/crawler/robots-checker.ts` ìƒì„±
- [ ] `lib/crawler/page-fetcher.ts` ìƒì„±
- [ ] `supabase/functions/crawl-pages/index.ts` ìƒì„±
- [ ] `components/page-analysis/CrawlResults.tsx` ìƒì„±
- [ ] robots.txt íŒŒì‹± ë¡œì§ í…ŒìŠ¤íŠ¸
- [ ] ë‹¤ì–‘í•œ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸
- [ ] íƒ€ì„ì•„ì›ƒ ë° ì—ëŸ¬ í•¸ë“¤ë§ í…ŒìŠ¤íŠ¸
- [ ] DB ì €ì¥ í™•ì¸

## ë‹¤ìŒ ë‹¨ê³„

Phase 3 ì™„ë£Œ í›„ â†’ **Phase 4: ê²½ìŸì‚¬ ë¶„ì„ ê°•í™”**

---

**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 3-4ì¼
**ë‚œì´ë„**: â­â­â­ (ë†’ìŒ - í¬ë¡¤ë§ ë³µì¡ë„)
